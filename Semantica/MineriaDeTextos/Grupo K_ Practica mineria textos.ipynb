{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c06cb09-820d-4c87-ad89-e96809f50395",
   "metadata": {},
   "source": [
    "### **GRUPO K - EJERCICIOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfc09f-7af1-44a8-8a20-53df55756eee",
   "metadata": {},
   "source": [
    "**Ejercicio 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d263235-b4ad-4550-b03c-decd58820c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejercicio 1.1: OK\n",
      "Ejercicio 1.2: OK\n",
      "Ejercicio 2.1: OK\n",
      "Ejercicio 2.2: OK\n",
      "Ejercicio 3.1: OK\n",
      "Ejercicio 3.2: OK\n",
      "Ejercicio 4.1: OK\n",
      "Ejercicio 4.2: OK\n",
      "Ejercicio 5.1: OK\n",
      "Ejercicio 5.2: OK\n",
      "Ejercicio 6.1: OK\n",
      "Ejercicio 6.2: OK\n",
      "Ejercicio 7.1: OK\n",
      "Ejercicio 7.2: OK\n",
      "Ejercicio 8.1: OK\n",
      "Ejercicio 8.2: OK\n",
      "Ejercicio 9.1: OK\n",
      "Ejercicio 9.2: OK\n",
      "Ejercicio 10.1: OK\n",
      "Ejercicio 10.2: OK\n",
      "Ejercicio 11.1: OK\n",
      "Ejercicio 11.2: OK\n",
      "Ejercicio 12.1: OK\n",
      "Ejercicio 12.2: OK\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Se pide codificar las siguientes expresiones regulares para encontrar\n",
    "# las palabras que se piden en cada ejercicio. Esto es, la expresion\n",
    "# regular debe ser capaz de encontrar palabras que cumplan la\n",
    "# especificacion y solo esas. Para simplificar la tarea del alumno, se\n",
    "# dejan dos ejemplos, una palabra que si cumple la especificacion y\n",
    "# otro que no. Los dos ultimos \n",
    "# ejercicios son opcionales. \n",
    "import re\n",
    "\n",
    "# Ejercicio 1: las palabras con letras minusculas\n",
    "\n",
    "\n",
    "patron1 = r'^[a-z]+$'\n",
    "if re.search(patron1,'estoesvalido'):\n",
    "    print (\"Ejercicio 1.1: OK\")\n",
    "if not re.search(patron1,'AQUI NO ESTA (de seguro)'):\n",
    "    print (\"Ejercicio 1.2: OK\")\n",
    "\n",
    "\n",
    "# Ejercicio 2: las palabras que empiecen por minuscula y luego solo tengan letras\n",
    "\n",
    "\n",
    "patron2 = r'^[a-z][A-z]+$'\n",
    "if re.search(patron2,'estoesvalido'):\n",
    "    print( \"Ejercicio 2.1: OK\")\n",
    "if not re.search(patron2,'estoNOESVALIDO7'):\n",
    "    print( \"Ejercicio 2.2: OK\")\n",
    "\n",
    "\n",
    "# Ejercicio 3: las palabras que tengan al menos dos numeros y lo demas sean letras\n",
    "\n",
    "patron3 = r'^\\w*\\d+\\w*\\d+\\w*$'\n",
    "if re.search(patron3,'7estoesvalido7'):\n",
    "    print (\"Ejercicio 3.1: OK\")\n",
    "if not re.search(patron3,'estoNOESVALIDO7'):\n",
    "    print (\"Ejercicio 3.2: OK\")\n",
    "\n",
    "# Ejercicio 4: Las palabras que contengan la cadena 111 al final\n",
    "\n",
    "\n",
    "patron4 = r'^\\w*[1]{3}$'\n",
    "if re.search(patron4,'7estoesvalido111'):\n",
    "    print (\"Ejercicio 4.1: OK\")\n",
    "if not re.search(patron4,'estoNOESVALIDO11117'):\n",
    "    print (\"Ejercicio 4.2: OK\")\n",
    "\n",
    "# Ejercicio 5: Las palabras que no tengan el parentesis al principio )\n",
    "\n",
    "\n",
    "patron5 = r'^[^(]\\S*$'\n",
    "if re.search(patron5,'7estoesvalido111___@@@@'):\n",
    "    print (\"Ejercicio 5.1: OK\")\n",
    "if not re.search(patron5,'(estoNOESVALIDO11117'):\n",
    "    print (\"Ejercicio 5.2: OK\")\n",
    "\n",
    "# Ejercicio 6: Las palabras que no contengan :( y esten formadas por\n",
    "# letras y espacios (DIFICIL)\n",
    "\n",
    "#^[:]*[^(]*$\n",
    "patron6 = r'^(?!.*:\\().*$'\n",
    "if re.search(patron6,'esto es valido :)'):\n",
    "    print (\"Ejercicio 6.1: OK\")\n",
    "if not re.search(patron6,'(esto me pone triste :('):\n",
    "    print (\"Ejercicio 6.2: OK\")\n",
    "\n",
    "# Ejercicio 7: Las palabras que contengan algun smiley de los siguientes :) ;)\n",
    "\n",
    "\n",
    "patron7 = r'^\\D*[:;][)]\\D*$'\n",
    "if re.search(patron7,'esto es valido :)'):\n",
    "    print (\"Ejercicio 7.1: OK\")\n",
    "if not re.search(patron7,'(esto no es valido :D'):\n",
    "    print (\"Ejercicio 7.2: OK\")\n",
    "\n",
    "\n",
    "# Ejercicio 8: Las palabras que acaben con algun smiley de los\n",
    "# siguientes :) ;) XD y empiecen con un OLA, los demas caracteres\n",
    "# pueden ser cualesquiera. \n",
    "\n",
    "\n",
    "patron8 = r'^(?:OLA).*([:;][)]|[X][D])$'\n",
    "if re.search(patron8,'OLA esto es valido XD'):\n",
    "    print (\"Ejercicio 8.1: OK\")\n",
    "if not re.search(patron8,'OLA esto no es valido 8)'):\n",
    "    print (\"Ejercicio 8.2: OK\")\n",
    "\n",
    "\n",
    "# Ejercicio 9: Cualquier etiqueta que pueda aparecer en XML \n",
    "\n",
    "\n",
    "patron9 = r'^<\\w*(?: \\w*=\"\\w*\")*>$'\n",
    "if re.search(patron9,'<etiqueta valida=\"YES\" corta=\"YES\">'):\n",
    "    print (\"Ejercicio 9.1: OK\")\n",
    "if not re.search(patron9,'<etiqueta valida=\"no\">>'):\n",
    "    print (\"Ejercicio 9.2: OK\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ejercicio 10: Cualquier direccion IP version 4 bien formada, esto\n",
    "# es, tres numeros entre 0 y 255, separados por un punto\n",
    "\n",
    "\n",
    "patron10 = r'^(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$'\n",
    "if re.search(patron10,'193.144.132.1'):\n",
    "    print (\"Ejercicio 10.1: OK\")\n",
    "if not re.search(patron10,'Esto no lo es 193.122.888.222'):\n",
    "    print (\"Ejercicio 10.2: OK\")\n",
    "\n",
    "\n",
    "# Ejercicio 11: (Opcional) Las palabras que sean direcciones de email\n",
    "\n",
    "\n",
    "patron11 = r'^\\S+[@]\\S+[\\.]\\S+$'\n",
    "if re.search(patron11,'esto@correcto.es'):\n",
    "    print (\"Ejercicio 11.1: OK\")\n",
    "if not re.search(patron11,'esto  incorrecto@.com'):\n",
    "    print (\"Ejercicio 11.2: OK\")\n",
    "\n",
    "\n",
    "\n",
    "# Ejercicio 12: Utilizar la opcion re.VERBOSE para poner comentarios\n",
    "# a  dos expresiones regulares.  \n",
    "\n",
    "patron121 = r'''\n",
    "    ^          \n",
    "    \\S+        # cualquier caracter que no es un espacio. correo\n",
    "    [@]        # simbolo @ \n",
    "    \\S+        # cualquier caracter que no es un espacio. tipo de correo\n",
    "    [\\.]       # simbolo .\n",
    "    \\S+        # cualquier caracter que no es un espacio. dominio\n",
    "    $          \n",
    "'''\n",
    "\n",
    "pattern_compiled1 = re.compile(patron121, re.VERBOSE)\n",
    "\n",
    "if pattern_compiled1.search('esto@correcto.es'):\n",
    "    print(\"Ejercicio 12.1: OK\")\n",
    "\n",
    "\n",
    "\n",
    "patron122 = r'''\n",
    "    ^         \n",
    "    (?:OLA)   # La palabra tiene que tener OLA al principio\n",
    "    .*        # culaquier cosa después\n",
    "    (         # Le sigue algo de lo siguiente: .) ;) o XD\n",
    "    [:;][)]  \n",
    "    |       \n",
    "    [X][D]  \n",
    "    )         \n",
    "    $         \n",
    "'''\n",
    "\n",
    "# Compile the pattern with re.VERBOSE flag\n",
    "pattern_compiled2 = re.compile(patron122, re.VERBOSE)\n",
    "\n",
    "# Test the pattern\n",
    "if pattern_compiled2.search('OLAdfhj:)'):\n",
    "    print(\"Ejercicio 12.2: OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07935c6b-4e20-4579-94af-a7901f7f21a9",
   "metadata": {},
   "source": [
    "**Ejercicio 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500015fa-f4be-4e68-899e-71c8cac3162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def procesar_reseñas(fichero_csv, carpeta_objetivo):\n",
    "    \"\"\"\n",
    "    Procesa las reseñas de un fichero CSV con etiquetas de sentimiento,\n",
    "    elimina etiquetas `<br /><br />` y guarda cada reseña en un archivo independiente\n",
    "    dentro de una carpeta objetivo.\n",
    "\n",
    "    Args:\n",
    "        fichero_csv (str): Ruta al fichero CSV con las reseñas.\n",
    "        carpeta_objetivo (str): Ruta a la carpeta donde se guardarán los archivos.\n",
    "    \"\"\"\n",
    "\n",
    "    # Expresión regular para eliminar etiquetas `<br /><br />`\n",
    "    regex_br = re.compile(r\"<br\\s*/?>\")\n",
    "    if not os.path.exists(carpeta_objetivo):\n",
    "        os.makedirs(carpeta_objetivo)\n",
    "\n",
    "    with open(fichero_csv, \"r\", encoding=\"utf-8\") as f_csv, open(\"log.txt\", \"w\", encoding=\"utf-8\") as f_log:\n",
    "        reader = csv.reader(f_csv, delimiter=\",\")\n",
    "        next(reader)  # Skip header row\n",
    "\n",
    "        for idx, row in enumerate(reader, start=1):\n",
    "            # Texto de la reseña\n",
    "            review, sentiment = row\n",
    "\n",
    "            # Eliminar etiquetas\n",
    "            review_sin_br = regex_br.sub(\"\", review)\n",
    "\n",
    "            # Construir nombre de archivo con índice, sentimiento y extensión\n",
    "            filename = f\"{carpeta_objetivo}/{idx}{sentiment}.txt\"\n",
    "\n",
    "            try:\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                    f_out.write(review_sin_br)\n",
    "            except Exception as e:\n",
    "                f_log.write(f\"Error al procesar reseña {idx}: {e}\\n\")\n",
    "\n",
    "fichero_csv = \"IMDB Dataset.csv\"\n",
    "carpeta_objetivo = \"script\"\n",
    "\n",
    "procesar_reseñas(fichero_csv, carpeta_objetivo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda77ed2-920e-48ec-855e-8d7ab41555cd",
   "metadata": {},
   "source": [
    "**Ejercicio 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655e286-ff30-4516-ad01-aba084c1ef7d",
   "metadata": {},
   "source": [
    "Diccionario de índices invertidos:\n",
    "d = dict()\n",
    " d”[breakthrough\"] = [1],\n",
    " d”[drug\"] = [1, 2],\n",
    " d”[for\"] = [1, 3, 4],\n",
    " d”[schizophrenia\"] = [1, 2, 3, 4],\n",
    " d”[new\"] = [2, 3, 4],\n",
    " d”[approach\"] = [3],\n",
    " d”[treatment\"] = [3],\n",
    " d”[of\"] = [3],\n",
    " d”[hopes\"] = [4],\n",
    " d”[patients\"] = [4]\n",
    "\n",
    "Matriz de incidencia: \n",
    "|           | D1 | D2 | D3 | D4 |\n",
    "|-----------|----|----|----|----|\n",
    "| breakthrough | 1  | 0  | 0  | 0  |\n",
    "| drug      | 1  | 1  | 0  | 1  |\n",
    "| for       | 1  | 0  | 1  | 1  |\n",
    "| schizophrenia | 1 | 1 | 1 | 1 |\n",
    "| new       | 0  | 1  | 1  | 0  |\n",
    "| approach  | 0  | 0  | 1  | 0  |\n",
    "| treatment | 0  | 0  | 1  | 0  |\n",
    "| of        | 0  | 0  | 1  | 0  |\n",
    "| hopes     | 0  | 0  | 0  | 1  |\n",
    "| patients  | 0  | 0  | 0  | 1  |\n",
    "\n",
    "\n",
    "Representa «for AND NOT (drug OR approach)» : \n",
    " (0, -1, 1 , 0, 0, -1, 0, 0,0,0 )\n",
    "\n",
    "La multiplicacion del vector por la matriz de incidencia da como resultado:  (0, -1, 0, 1 )\n",
    "Donde los números positivos representan la coincidencia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bda893-44d3-48ef-8eb9-3c63fe7f12ae",
   "metadata": {},
   "source": [
    "**Ejercicio 4**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab3987a6-e2ba-42e6-a1ed-d9d38cacd0f4",
   "metadata": {},
   "source": [
    "BRUTUS = B\n",
    "CAESAR = C\n",
    "ANTONY = A\n",
    "CLEOPATRA = CL\n",
    "(B OR C) AND NOT (A OR CL)\n",
    "(B OR C) AND (NOT A AND NOT CL)\n",
    "(B AND NOT A AND NOT CL) OR (C AND NOT A AND NOT CL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc95e8-7ede-47e7-a2c4-851d6e91e2cd",
   "metadata": {},
   "source": [
    "**Ejercicio 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05575964-596f-4af0-8496-7459d38aeaaa",
   "metadata": {},
   "source": [
    "Lo primero que necesitamos hacer es una lista que muestre la frecuencia con la que aparecen los términos en los documentos. Una vez tenemos esta lista, como lo que necesitamos hacer es una conjunción de términos, es decir, necesitamos encontrar los documentos que contienen el termino 1+el termino 2 + el termino 3… entonces comenzaremos a buscar los documentos que contienen esos términos empezando por el termino que aparece en el menor numero de documentos. Esto nos ayudara a descartar los documentos que solo contienen uno de los términos, pero no los otros. La idea es que si un termino aparece solo en 5 documentos y otro aparece en 100, al comenzar por el que aparece en solo 5 nos ayuda a detectar cual de esos 5 contiene los dos términos. Si lo hiciésemos al revés tendríamos que buscar en los 100 documentos cuales contienen los dos términos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e3259-fa20-42e8-bd99-86f69d37f218",
   "metadata": {},
   "source": [
    "**Ejercicio 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d9cde-e6a3-48e7-8366-395ab02490ed",
   "metadata": {},
   "source": [
    "Sabemos que al menos hay aprox. 316000 documentos en total, por lo que podemos asumir que al menos puede haber aprox. 229000 documentos que no contienen “kaleidoscope” esto es mayor que las 213000 apariciones de la palabra “eyes”, así que sería mas eficiente comenzar buscando los documentos que contienen esta palabra. Una vez tenemos los documentos que contienen esta palabra podemos ahora buscar entre ellos los que no contienen “kaleidoscope”. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682e372-a31f-4d06-9139-14d8790b3a1d",
   "metadata": {},
   "source": [
    "**Ejercicio 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2cbfdc-dd19-428c-856e-058963efc8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coincidences: [1, 375, 805, 2349, 5026, 5741, 6151, 6501, 6706, 8167, 10466, 11893, 14363, 15169, 15587, 15794, 19259, 19815, 20062, 20683, 22019, 22556, 25684, 26718, 28854, 29041, 29242, 30115, 31847, 33811, 34394, 34462, 35173, 37005, 37380, 39079, 39334, 39368, 39429, 40193, 40208, 42084, 42868, 43669, 45035, 45222, 45815, 46402, 46562, 46817, 47105, 47287, 47984, 48297, 49668]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_word(fichero_csv, palabra1, palabra2):\n",
    "    dictionary = dict()\n",
    "    dictionary[palabra1] = []\n",
    "    dictionary[palabra2] = []\n",
    "    dictionary[f\"{palabra1}-{palabra2}\"] = []\n",
    "\n",
    "\n",
    "    with open(fichero_csv, newline='', encoding=\"utf-8\") as csvfile:\n",
    "        i = 0\n",
    "        for linea in csvfile:\n",
    "            i += 1\n",
    "            if palabra1 in linea:\n",
    "                dictionary[palabra1].append(i)\n",
    "            if palabra2 in linea:\n",
    "                dictionary[palabra2].append(i)\n",
    "    with open(fichero_csv, newline='', encoding=\"utf-8\") as csvfile2:\n",
    "        i = 0\n",
    "        if len(dictionary[palabra1]) <= len(dictionary[palabra2]):\n",
    "            for linea in csvfile2:\n",
    "                i += 1\n",
    "                if i in dictionary[palabra1] and palabra2 in linea:\n",
    "                    dictionary[f\"{palabra1}-{palabra2}\"].append(i)\n",
    "        else:\n",
    "            for linea in csvfile2:\n",
    "                i += 1\n",
    "                if i in dictionary[palabra2] and palabra1 in linea:\n",
    "                    dictionary[f\"{palabra1}-{palabra2}\"].append(i)\n",
    "    return dictionary[f\"{palabra1}-{palabra2}\"]\n",
    "\n",
    "coincidences = get_word(\"IMDB_raiz.csv\", \"brutal\", \"prison\")\n",
    "\n",
    "\n",
    "print(f\"Coincidences: {coincidences}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630d0f0-7e79-433b-93b2-073632f1a6b6",
   "metadata": {},
   "source": [
    "**Ejercicio 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94fa622-c7b1-4166-8e17-aa3a335f0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(consulta, doc):\n",
    "    #consulta: objeto que contiene la lista de palabras y que set admite como listas \n",
    "    #doc: string formado por varias palabras\n",
    "    setcon = set(consulta)\n",
    "    setdoc = set(re.findall(r\"[\\w']+\", doc))\n",
    "    coef = len(setcon.intersection(setdoc))/len(setcon.union(setdoc))\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57534812-c38d-498b-9f26-3ca536869aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005050505050505051\n"
     ]
    }
   ],
   "source": [
    "with open(fichero_csv, newline='', encoding=\"utf-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    lines = list(reader)\n",
    "    doc = lines[coincidences[0]][0]\n",
    "    consulta = ['brutal', 'prison']\n",
    "    print(Jaccard(consulta, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b31bf-bfe4-4393-9ce4-810defab5958",
   "metadata": {},
   "source": [
    "**Ejercicio 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb237ea-c23d-4f63-8efb-d04a8d8cbe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor: 4.1289749842731975 Posición: 49117\n",
      "Valor: 3.751273298986431 Posición: 24948\n",
      "Valor: 3.6632075681353995 Posición: 7219\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_pesos(fichero_csv, palabra1, palabra2, num_higher):\n",
    "\n",
    "    fila_palabra1 = []\n",
    "    fila_palabra2 = []\n",
    "    with open(fichero_csv, newline='', encoding=\"utf-8\") as csvfile:\n",
    "        num_docs = np.array(csvfile.readlines()).size\n",
    "        incidencias_1 = 0\n",
    "        incidencias_2 = 0\n",
    "    with open(fichero_csv, newline='', encoding=\"utf-8\") as csvfile:\n",
    "        for line in csvfile:\n",
    "            if palabra1 in line:\n",
    "                incidencias_1 += 1\n",
    "            if palabra2 in line:\n",
    "                incidencias_2 += 1\n",
    "    with open(fichero_csv, newline='', encoding=\"utf-8\") as csvfile:\n",
    "        for line in csvfile:\n",
    "            ocurrencias_1 = line.count(palabra1)\n",
    "            ocurrencias_2 = line.count(palabra2)\n",
    "\n",
    "            peso_1 = (1 + np.log10(ocurrencias_1))*np.log10(num_docs/incidencias_1) if ocurrencias_1 != 0 else 0\n",
    "            fila_palabra1.append(peso_1)\n",
    "            peso_2 = (1 + np.log10(ocurrencias_2))*np.log10(num_docs/incidencias_2) if ocurrencias_2 != 0 else 0\n",
    "            fila_palabra2.append(peso_2)\n",
    "\n",
    "        \n",
    "    valoraciones = np.array(fila_palabra1) + np.array(fila_palabra2)\n",
    "\n",
    "    # Usa sorted() para ordenar la lista en orden descendente y obtén los tres primeros elementos\n",
    "    higher = sorted(enumerate(valoraciones), key=lambda x: x[1], reverse=True)[:num_higher]\n",
    "\n",
    "    # Imprime los tres valores más altos y sus posiciones\n",
    "    for posicion, valor in higher:\n",
    "        print(\"Valor:\", valor, \"Posición:\", posicion)\n",
    "\n",
    "get_pesos(\"IMDB_raiz.csv\", \"humor\", \"oscar\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44c4aa-cb43-40c6-8ed3-fb71b1f24bb0",
   "metadata": {},
   "source": [
    "**Ejercicio 10**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bd5b8ea-d100-4947-89f4-bc6ea4e22319",
   "metadata": {},
   "source": [
    "Al analizar el calculo de TF-IDF para el caso planteado tendriamos para cada documento el calculo: (1 + log(1)) *log(2/2) = 0\n",
    "Por lo que esperariamos valores 0, pero al ver la documentacion de TfidfTransformer de Sklearn podemos encontrar que la opción use_idf=False no considera el termino de IDF, por lo que tendriamos (1 + log(1)) = 1, lo que es congruente con nuestro resultado.\n",
    "Pero ocurre que si usamos use_idf = True, por como esta planteado en la documentacion, la formula para nuestro caso pasaria a ser: (1 + log(1)) *(log(2/2)+1) = 1\n",
    "Por lo que continuariamos teniendo el valor de 1 al usar TfidfTransformer, pero que teoricamente sería 0.\n",
    "Ya que plantea idf = log [n/df(t)] +1 y en caso de usar smooth_idf pasaria a ser: idf = log [n+1/df(t)+1] +1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393166ea-2329-47b4-8de2-b6dd7c566cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          humor     oscar       Sum\n",
      "35717  0.126101  0.108947  0.235049\n",
      "37405  0.118113  0.102046  0.220160\n",
      "26857  0.143966  0.062191  0.206157\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "stop_words = ['one', 'of', 'a', 'the', 'them', 'this', 'they', 'that', 'right', 'what', 'why', 'who']#Descartar otra palabra\n",
    "vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)#, stop_words = stop_words)\n",
    "with open(\"IMDB_raiz.csv\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    corpus = np.array(csvfile.readlines())\n",
    "\n",
    "#corpus = [\"Esto\", \"Esto\"]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#print(X.toarray()) # [[0.], [0.]]\n",
    "#pd.DataFrame(X.todense(),columns=vectorizer.get_feature_names())\n",
    "# Select the desired terms (replace with actual terms)\n",
    "desired_terms = ['humor', 'oscar']\n",
    "\n",
    "# Create DataFrame with selected terms and a sum column\n",
    "df = pd.DataFrame(X.todense()[:, :2], columns=desired_terms)\n",
    "\n",
    "df = df.loc[(df['humor'] != 0) & (df['oscar'] != 0)]\n",
    "df['Sum'] = df[desired_terms[0]] + df[desired_terms[1]]\n",
    "df = df.sort_values('Sum', ascending=False)\n",
    "\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212a88f-0746-46f2-8319-9d25f53978f4",
   "metadata": {},
   "source": [
    "Obtenemos un resultado distinto, que es debido a la normalizacion que se realiza en TfidfVectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
